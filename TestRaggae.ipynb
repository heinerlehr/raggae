{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from loguru import logger\n",
    "from ragstore import RAGStore\n",
    "from ragquery import RAGQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.add('run.log', rotation=\"1 week\", retention=\"4 weeks\")    # Once the file is too old, it's rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.environ['HOME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiating the RAG store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of ragquery failed: Traceback (most recent call last):\n",
      "  File \"/app/.venv/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/app/.venv/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/importlib/__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"/app/ragquery.py\", line 47\n",
      "    If the answer is not in the context or the hisotry, do not make up answers, just say that you don't know.\n",
      "                                                                                                     ^\n",
      "SyntaxError: unterminated string literal (detected at line 47)\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "rs = RAGStore(debug=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding some files to the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-15 13:44:01.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_file\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mProcessing of file test/RAG Project Dataset/1706.03762v7.pdf starts\u001b[0m\n",
      "\u001b[32m2025-02-15 13:44:01.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_file\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mProcessing of file test/RAG Project Dataset/2005.11401v4.pdf starts\u001b[0m\n",
      "\u001b[32m2025-02-15 13:44:01.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_file\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mProcessing of file test/RAG Project Dataset/2005.14165v4.pdf starts\u001b[0m\n",
      "\u001b[32m2025-02-15 13:44:01.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_file\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mProcessing of file test/RAG Project Dataset/sample_document.pdf starts\u001b[0m\n",
      "\u001b[32m2025-02-15 13:44:01.559\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_file\u001b[0m:\u001b[36m168\u001b[0m - \u001b[33m\u001b[1mFile test/RAG Project Dataset/sample_document.pdf already in store\u001b[0m\n",
      "\u001b[32m2025-02-15 13:44:01.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_folder\u001b[0m:\u001b[36m335\u001b[0m - \u001b[1mFile test/RAG Project Dataset/sample_document.pdf processed\u001b[0m\n",
      "\u001b[32m2025-02-15 13:44:01.576\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_file\u001b[0m:\u001b[36m168\u001b[0m - \u001b[33m\u001b[1mFile test/RAG Project Dataset/2005.11401v4.pdf already in store\u001b[0m\n",
      "\u001b[32m2025-02-15 13:44:01.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_folder\u001b[0m:\u001b[36m335\u001b[0m - \u001b[1mFile test/RAG Project Dataset/2005.11401v4.pdf processed\u001b[0m\n",
      "\u001b[32m2025-02-15 13:44:01.624\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_file\u001b[0m:\u001b[36m168\u001b[0m - \u001b[33m\u001b[1mFile test/RAG Project Dataset/1706.03762v7.pdf already in store\u001b[0m\n",
      "\u001b[32m2025-02-15 13:44:01.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_folder\u001b[0m:\u001b[36m335\u001b[0m - \u001b[1mFile test/RAG Project Dataset/1706.03762v7.pdf processed\u001b[0m\n",
      "\u001b[32m2025-02-15 13:44:02.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_file\u001b[0m:\u001b[36m174\u001b[0m - \u001b[1mExtracting tables\u001b[0m\n",
      "\u001b[32m2025-02-15 13:59:26.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_file\u001b[0m:\u001b[36m184\u001b[0m - \u001b[1mTables extracted after 924.6 seconds\u001b[0m\n",
      "\u001b[32m2025-02-15 13:59:26.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_file\u001b[0m:\u001b[36m187\u001b[0m - \u001b[1mExtracting text elements\u001b[0m\n",
      "\u001b[32m2025-02-15 14:09:56.891\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_file\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mTexts extracted after 1555.4 seconds\u001b[0m\n",
      "\u001b[32m2025-02-15 14:09:56.894\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_file\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mExtracting figures\u001b[0m\n",
      "\u001b[32m2025-02-15 14:09:59.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_file\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1mFigures extracted after 1557.6 seconds\u001b[0m\n",
      "\u001b[32m2025-02-15 14:09:59.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_file\u001b[0m:\u001b[36m212\u001b[0m - \u001b[1mSummarising document\u001b[0m\n",
      "\u001b[32m2025-02-15 14:10:12.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36mset_context\u001b[0m:\u001b[36m591\u001b[0m - \u001b[1mSummarising the document costed 70567 tokens\u001b[0m\n",
      "\u001b[32m2025-02-15 14:10:12.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_file\u001b[0m:\u001b[36m214\u001b[0m - \u001b[1mDocument summarised after 1571.2 seconds\u001b[0m\n",
      "\u001b[32m2025-02-15 14:10:12.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_file\u001b[0m:\u001b[36m216\u001b[0m - \u001b[1mFile test/RAG Project Dataset/2005.14165v4.pdf partitioned into 168 elements in 1571.2 seconds\u001b[0m\n",
      "\u001b[32m2025-02-15 14:24:01.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_file\u001b[0m:\u001b[36m302\u001b[0m - \u001b[1mProcessing of file test/RAG Project Dataset/2005.14165v4.pdf completed in 2400.3 seconds\u001b[0m\n",
      "\u001b[32m2025-02-15 14:24:01.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragstore\u001b[0m:\u001b[36madd_folder\u001b[0m:\u001b[36m335\u001b[0m - \u001b[1mFile test/RAG Project Dataset/2005.14165v4.pdf processed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "rs.add_folder('test/RAG Project Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying the RAG store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rq = RAGQuery(document_store=rs.get_document_store(), vector_store=rs.get_vector_store(), debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer model, introduced in the paper \"Attention Is All You Need,\" is a groundbreaking neural network architecture designed for sequence transduction tasks, such as machine translation. Here is a detailed explanation of its architecture, key innovations, and performance:\n",
      "\n",
      "### Architecture\n",
      "The Transformer model is based on an encoder-decoder structure:\n",
      "- **Encoder**: It consists of six identical layers, each comprising a multi-head self-attention mechanism and a position-wise feed-forward network. These layers incorporate residual connections and layer normalization.\n",
      "- **Decoder**: Similarly, the decoder has six identical layers, but with an additional sub-layer for multi-head attention over the encoder's output. This setup prevents future position attention to maintain the auto-regressive property of the model.\n",
      "\n",
      "### Key Innovations\n",
      "1. **Attention Mechanisms**: The Transformer relies entirely on attention mechanisms, eliminating the need for recurrent and convolutional layers. This design allows for significant parallelization and faster training times.\n",
      "2. **Multi-Head Attention**: This allows the model to attend to information from different representation subspaces at different positions, enhancing its ability to model complex dependencies.\n",
      "3. **Positional Encoding**: Since the model does not use recurrence, positional encodings are added to input embeddings to maintain the order of sequences.\n",
      "4. **Self-Attention**: Employed to relate different positions within a single sequence, self-attention is crucial for tasks like reading comprehension and summarization.\n",
      "\n",
      "### Performance in Machine Translation\n",
      "The Transformer achieves state-of-the-art performance in machine translation tasks:\n",
      "- It scored 28.4 BLEU on the WMT 2014 English-to-German task.\n",
      "- It scored 41.8 BLEU on the English-to-French task.\n",
      "These scores surpass previous models while requiring fewer computational resources, demonstrating the model's efficiency and effectiveness.\n",
      "\n",
      "### Sources\n",
      "The information provided is based on the context from the paper \"Attention Is All You Need\" and related discussions on the Transformer model's architecture and performance.\n"
     ]
    }
   ],
   "source": [
    "question = 'Explain transformers in less than 5 sentences and provide a list of your sources.'\n",
    "answer = rq.query(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the databases\n",
    "If it is desired to start from scratch, the following will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = False\n",
    "if clean:\n",
    "    rs.get_vector_store ().clean()\n",
    "    rs.get_document_store().clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
